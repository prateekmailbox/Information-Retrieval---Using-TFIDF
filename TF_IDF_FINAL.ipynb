{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "TF-IDF (1).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw8KNPcpf2Nf"
      },
      "source": [
        "# TF-IDF PROJECT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pphb-9Rf2Ng"
      },
      "source": [
        "## INDEX:\n",
        "###   1) Importing Libraries\n",
        "###   2) Extracting files\n",
        "###   3) Tokenization of file\n",
        "###   4) Stemming of file\n",
        "###   5) Removal of StopWords\n",
        "###   6) Removal of Punctuations\n",
        "###   7) Preprocessing the corpus and cal of term freq\n",
        "###   8) Cal Document Freq\n",
        "###   9) Cal TF-IDF value\n",
        "### 10) Query Input and Preprocessing\n",
        "### 11) TF-IDF for Query\n",
        "### 12) Result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rl49Upsf2Ng"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX0KuCdXf2Ng"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk    #nltk Library used for Data preprocessing\n",
        "from nltk import word_tokenize # Used for Tokenization of file\n",
        "import pathlib # extract the file from desktop\n",
        "from nltk.stem import PorterStemmer # Stem the file\n",
        "from nltk.corpus import stopwords  # Stopword removal\n",
        "from nltk.tokenize import word_tokenize\n",
        "import math # \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNIAvjYGf2Nh"
      },
      "source": [
        " ## Extracting English StopWords from stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Mkis9vf2Nh",
        "outputId": "a2a8d67c-8a8f-4bef-9a56-0b073c8bb5b6"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords=stopwords.words('English')\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq1ao_bzf2Nh"
      },
      "source": [
        "## PreProcessing a Sample Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMN78G5rf2Nh"
      },
      "source": [
        "## Importing files from folder name 'sample' which has 11 mails\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Of2sVQBf2Ni"
      },
      "source": [
        "file=[]\n",
        "for path in pathlib.Path(\"D://desktop//sample\").iterdir():\n",
        "    if path.is_file():\n",
        "        current_file = open(path, \"r\")\n",
        "        file.append(current_file.read())\n",
        "        current_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN7xQXQPf2Ni",
        "outputId": "533cdb6c-adb3-4f03-f3cc-bdfe21d91740"
      },
      "source": [
        "file[0] #first File\n",
        "#print(len(file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Message-ID: <18145471.1075848342989.JavaMail.evans@thyme>\\nDate: Tue, 28 Nov 2000 06:56:00 -0800 (PST)\\nFrom: suresh.raghavan@enron.com\\nTo: brad.richter@enron.com\\nSubject: Ross Mesquita's PRC\\nCc: harry.arora@enron.com\\nMime-Version: 1.0\\nContent-Type: text/plain; charset=us-ascii\\nContent-Transfer-Encoding: 7bit\\nBcc: harry.arora@enron.com\\nX-From: Suresh Raghavan\\nX-To: Brad Richter\\nX-cc: Harry Arora\\nX-bcc: \\nX-Folder: \\\\Harpreet_Arora_Nov2001\\\\Notes Folders\\\\All documents\\nX-Origin: ARORA-H\\nX-FileName: harora.nsf\\n\\nAttached is a one page summary on Ross.  Look forward to speaking with you \\nthis afternoon at 4:15pm.  Thanks.  SR:-)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcndCUeUf2Ni"
      },
      "source": [
        "## Tokenization of files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmAhi35zf2Ni",
        "outputId": "93e202c7-cb00-48d8-bd91-99c9b60d553f"
      },
      "source": [
        "token_file = []\n",
        "for x in file:                                #select one file at a time from the folder and tokekize and append to token_file[]\n",
        "    token_file.append(word_tokenize(x))\n",
        "token_file[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Message-ID',\n",
              " ':',\n",
              " '<',\n",
              " '18145471.1075848342989.JavaMail.evans',\n",
              " '@',\n",
              " 'thyme',\n",
              " '>',\n",
              " 'Date',\n",
              " ':',\n",
              " 'Tue',\n",
              " ',',\n",
              " '28',\n",
              " 'Nov',\n",
              " '2000',\n",
              " '06:56:00',\n",
              " '-0800',\n",
              " '(',\n",
              " 'PST',\n",
              " ')',\n",
              " 'From',\n",
              " ':',\n",
              " 'suresh.raghavan',\n",
              " '@',\n",
              " 'enron.com',\n",
              " 'To',\n",
              " ':',\n",
              " 'brad.richter',\n",
              " '@',\n",
              " 'enron.com',\n",
              " 'Subject',\n",
              " ':',\n",
              " 'Ross',\n",
              " 'Mesquita',\n",
              " \"'s\",\n",
              " 'PRC',\n",
              " 'Cc',\n",
              " ':',\n",
              " 'harry.arora',\n",
              " '@',\n",
              " 'enron.com',\n",
              " 'Mime-Version',\n",
              " ':',\n",
              " '1.0',\n",
              " 'Content-Type',\n",
              " ':',\n",
              " 'text/plain',\n",
              " ';',\n",
              " 'charset=us-ascii',\n",
              " 'Content-Transfer-Encoding',\n",
              " ':',\n",
              " '7bit',\n",
              " 'Bcc',\n",
              " ':',\n",
              " 'harry.arora',\n",
              " '@',\n",
              " 'enron.com',\n",
              " 'X-From',\n",
              " ':',\n",
              " 'Suresh',\n",
              " 'Raghavan',\n",
              " 'X-To',\n",
              " ':',\n",
              " 'Brad',\n",
              " 'Richter',\n",
              " 'X-cc',\n",
              " ':',\n",
              " 'Harry',\n",
              " 'Arora',\n",
              " 'X-bcc',\n",
              " ':',\n",
              " 'X-Folder',\n",
              " ':',\n",
              " '\\\\Harpreet_Arora_Nov2001\\\\Notes',\n",
              " 'Folders\\\\All',\n",
              " 'documents',\n",
              " 'X-Origin',\n",
              " ':',\n",
              " 'ARORA-H',\n",
              " 'X-FileName',\n",
              " ':',\n",
              " 'harora.nsf',\n",
              " 'Attached',\n",
              " 'is',\n",
              " 'a',\n",
              " 'one',\n",
              " 'page',\n",
              " 'summary',\n",
              " 'on',\n",
              " 'Ross',\n",
              " '.',\n",
              " 'Look',\n",
              " 'forward',\n",
              " 'to',\n",
              " 'speaking',\n",
              " 'with',\n",
              " 'you',\n",
              " 'this',\n",
              " 'afternoon',\n",
              " 'at',\n",
              " '4:15pm',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.',\n",
              " 'SR',\n",
              " ':',\n",
              " '-',\n",
              " ')']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrcjPtLaf2Ni"
      },
      "source": [
        "## Stemming of files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruE37lWaf2Ni"
      },
      "source": [
        "stem_file = []\n",
        "ps = PorterStemmer()\n",
        "for x in token_file:                      #Select one file at a time from token_file[] and perfom stemmin and appned to stem_file[]\n",
        "    temp=[]\n",
        "    for i in x:\n",
        "        temp.append(ps.stem(i))\n",
        "        \n",
        "    stem_file.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGayNBuTf2Nj"
      },
      "source": [
        "## Removal of stop words from stemmed file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u9lBRFWf2Nj"
      },
      "source": [
        "stems_without_sw = []\n",
        "for x in stem_file:\n",
        "    stems_without_sw .append([word for word in x if not word in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_-H5lGBf2Nj"
      },
      "source": [
        "## Removal of Punctuation from the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVlFSwNMf2Nj"
      },
      "source": [
        "stems_without_sw_punct = []\n",
        "for x in stems_without_sw:\n",
        "    stems_without_sw_punct .append( [word for word in x if word.isalnum()])\n",
        "#stems_without_sw_punct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT39Qnqnf2Nj"
      },
      "source": [
        "## Pre-Processed File"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH9utAhbf2Nj"
      },
      "source": [
        "# file 1 \n",
        "#file[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgqCeGIDf2Nj"
      },
      "source": [
        "#file after tokenization\n",
        "#token_file[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbAoAFcnf2Nj"
      },
      "source": [
        "#file after stemming\n",
        "#stem_file[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7xr_e7lf2Nk"
      },
      "source": [
        "#stem_without_sw\n",
        "#stems_without_sw[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY-HzhHKf2Nk"
      },
      "source": [
        "#stems_without_sw\n",
        "#stems_without_sw_punct[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72TxZXT8f2Nk"
      },
      "source": [
        "## Applying TF-IDF On Corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xA-QSo5f2Nk"
      },
      "source": [
        "## Importing all the file paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wtUCZp5f2Nk"
      },
      "source": [
        "import os\n",
        "file_names = []                                               \n",
        "for path, direc, files in os.walk(\"D://desktop//sem 6th//IR//corpus\"):              \n",
        "    for i in files:                                           \n",
        "        file_names.append(path+\"/\"+i)  \n",
        "num_of_doc=float(len(file_names)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fguj06RDf2Nk",
        "outputId": "6f4bc1c6-422e-4c71-c2b1-41059447bdc7"
      },
      "source": [
        "file_names[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'D://desktop//sem 6th//IR//corpus\\\\arora-h\\\\all_documents/1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHqmxD7-f2Nk",
        "outputId": "75681c6b-d2d9-44a7-d815-98995b394c02"
      },
      "source": [
        "num_of_doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4703.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9UmczEPf2Nk"
      },
      "source": [
        "## Applying Preprocessing and Term Freq for Each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWWUUphSf2Nk"
      },
      "source": [
        "length_of_docs={}\n",
        "term ={}\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "for x in file_names:                                      #One file at a time from the corpus\n",
        "    file_content = open(x).read()\n",
        "    token_file=(word_tokenize(file_content))              #Applying Tokenization\n",
        "   # print(token_file)                                       \n",
        " \n",
        "    stem_file = []                                        #Applying Stemming\n",
        "    for i in token_file:\n",
        "        stem_file.append(ps.stem(i))\n",
        "        \n",
        "   \n",
        "    stems_without_sw = ([word for word in stem_file if not word in stopwords])   #StopWord Removal  \n",
        "     \n",
        "    stems_without_sw_punct = ( [word for word in stems_without_sw if word.isalnum()])  #Punctuation Removal\n",
        "    \n",
        "    pre_file=stems_without_sw_punct\n",
        "    \n",
        "    #print(pre_file)\n",
        "    length_of_docs[x]=len(pre_file)            \n",
        "    \n",
        "    for i in pre_file:                                  # for each word in mail storing document it is present \n",
        "                                                        # and also the frequency of the term \n",
        "        if i not in term:\n",
        "            term[i]={}\n",
        "            term[i][x]=1\n",
        "        else:\n",
        "            if x in term[i]:\n",
        "                term[i][x] += 1\n",
        "            else:\n",
        "                term[i][x]=1 \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcKks3wFf2Nl"
      },
      "source": [
        "![Screenshot%20%28759%29.png](attachment:Screenshot%20%28759%29.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrCp6cWnf2Nl"
      },
      "source": [
        "#term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owbmvcAef2Nl"
      },
      "source": [
        "## Cal Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZFakLhaf2Nl"
      },
      "source": [
        "doc_freq={}\n",
        "\n",
        "for i in term:\n",
        "    doc_freq[i]= ( len(term[i]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOdyOrBFf2Nl"
      },
      "source": [
        "#doc_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgxX3hJof2Nl"
      },
      "source": [
        "## Applying TF-IDF value for each term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r04ASLkzf2Nl"
      },
      "source": [
        "for i in term:                                                                           \n",
        "    for j in term[i]:                                                                       \n",
        "        (term[i])[j]=(math.log10(1+(term[i])[j])) * math.log10(num_of_doc/doc_freq[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JlGX1Nnf2Nm"
      },
      "source": [
        "![Screenshot%20%28760%29.png](attachment:Screenshot%20%28760%29.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6Iwby7f2Nm"
      },
      "source": [
        "#term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFx5wU5hf2Nm"
      },
      "source": [
        "## Query Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNEHml0Af2Nm"
      },
      "source": [
        "## Input Query and PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLCT-fRBf2Nm",
        "outputId": "3f195b2c-072f-4c04-e500-48a512b37a9b"
      },
      "source": [
        "query = input(\"Enter your query: \")    \n",
        "\n",
        "token_query=(word_tokenize(query))\n",
        "stem_query = [] \n",
        "\n",
        "for i in token_query:\n",
        "        stem_query.append(ps.stem(i))\n",
        "        \n",
        "query_stems_without_sw = ([word for word in stem_query if not word in stopwords]) \n",
        "\n",
        "query_stems_without_sw_punct = ( [word for word in query_stems_without_sw if word.isalnum()])\n",
        "\n",
        "pre_query_file=query_stems_without_sw_punct\n",
        "\n",
        "pre_query_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your query: post the message helpul\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['post', 'messag', 'helpul']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtERTAzIf2Nm"
      },
      "source": [
        "## Cal of term frequency of input query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7vmN-uEf2Nm"
      },
      "source": [
        "query_term={}\n",
        "\n",
        "for i in pre_query_file:\n",
        "    if i not in query_term:\n",
        "        query_term[i] = 1\n",
        "    else:\n",
        "        query_term[i] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WenyP705f2Nn",
        "outputId": "c885d3b6-9019-4a72-80cf-324c5a6d8d84"
      },
      "source": [
        "query_term"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'post': 1, 'messag': 1, 'helpul': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb2uxP5_f2Nn"
      },
      "source": [
        "## Normalization of Term Freq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqcdaru0f2Nn"
      },
      "source": [
        "for i in query_term:\n",
        "    query_term[i]=math.log10(1+query_term[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBgxy9Eyf2Nn",
        "outputId": "c2e2fb3f-9809-43e4-fb8f-36d9db4c23b7"
      },
      "source": [
        "query_term"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'post': 0.3010299956639812,\n",
              " 'messag': 0.3010299956639812,\n",
              " 'helpul': 0.3010299956639812}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b588g4pf2No"
      },
      "source": [
        "## Cal tf-idf value for each document consisting query words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zegugGR9f2No"
      },
      "source": [
        "retrive = {}\n",
        "\n",
        "for i in query_term:\n",
        "    if i in term:\n",
        "        for j in term[i]:\n",
        "            if j not in retrive:\n",
        "                retrive[j] = query_term[i]*term[i][j]\n",
        "            else:\n",
        "                retrive[j] += query_term[i]*term[i][j]\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raSdAGikf2No"
      },
      "source": [
        "#retrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i43D1VCVf2No"
      },
      "source": [
        "## Sorting Based on TF-IDF Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz0OtKXZf2Np"
      },
      "source": [
        "result = []\n",
        "\n",
        "result= sorted(retrive.items(), key=lambda x: x[1],  reverse=True)\n",
        "#result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Maq0iV8f2Np"
      },
      "source": [
        "## Retriving Top 10 Relevant Mails For The Input Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-2NTblAf2Np",
        "outputId": "1fb12049-ac71-47b7-9485-d9b05d85d792"
      },
      "source": [
        "i=0\n",
        "while i<len(result) and i<10:\n",
        "    print(result[i])\n",
        "    i += 1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/1001', 0.4204224265192071)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/930', 0.3423599898332569)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/932', 0.3423599898332569)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/935', 0.3423599898332569)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/1712', 0.29827819450179904)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/668', 0.29827819450179904)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\discussion_threads/14', 0.29827819450179904)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/495', 0.2953447734465818)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\all_documents/733', 0.27912425249236034)\n",
            "('D://desktop//sem 6th//IR//corpus\\\\bass-e\\\\deleted_items/319', 0.27912425249236034)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xErJSRDaf2Np"
      },
      "source": [
        "# THANK YOU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCH7ssyTf2Np"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}